{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author name:** Thanh Chung Nguyen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Write code to SparkSession is created using a SparkConf object, which would use two local cores with a proper application name, and use UTC as the timezone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark import SparkConf\n",
    "\n",
    "master = \"local[2]\"\n",
    "spark_conf = SparkConf().setMaster(master)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Analysis in Spark\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", 'UTC')\\\n",
    "    .config(conf=spark_conf)\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Use the same topic names from the Kafka producer in Task 1, ingest the streaming data into Spark Streaming and assume all data coming in String format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_bu = \"bureau\"\n",
    "df_bu = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", topic_bu) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_cus = \"customer\"\n",
    "df_cus = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", topic_cus) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bu = df_bu.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "df_cus = df_cus.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_bu = ArrayType(StructType([    \n",
    "    StructField('ID', StringType(), True), \n",
    "    StructField('SELF-INDICATOR', StringType(), True),\n",
    "    StructField('MATCH-TYPE', StringType(), True), \n",
    "    StructField('ACCT-TYPE', StringType(), True),\n",
    "    StructField('CONTRIBUTOR-TYPE', StringType(), True), \n",
    "    StructField('DATE-REPORTED', StringType(), True),\n",
    "    StructField('OWNERSHIP-IND', StringType(), True), \n",
    "    StructField('ACCOUNT-STATUS', StringType(), True),\n",
    "    StructField('DISBURSED-DT', StringType(), True), \n",
    "    StructField('CLOSE-DT', StringType(), True),\n",
    "    StructField('LAST-PAYMENT-DATE', StringType(), True),\n",
    "    StructField('CREDIT-LIMIT/SANC AMT', StringType(), True), \n",
    "    StructField('DISBURSED-AMT/HIGH CREDIT', StringType(), True),\n",
    "    StructField('INSTALLMENT-AMT', StringType(), True), \n",
    "    StructField('CURRENT-BAL', StringType(), True),\n",
    "    StructField('INSTALLMENT-FREQUENCY', StringType(), True), \n",
    "    StructField('OVERDUE-AMT', StringType(), True), \n",
    "    StructField('WRITE-OFF-AMT', StringType(), True),\n",
    "    StructField('ASSET_CLASS', StringType(), True), \n",
    "    StructField('REPORTED DATE - HIST', StringType(), True),\n",
    "    StructField('DPD - HIST', StringType(), True), \n",
    "    StructField('CUR BAL - HIST', StringType(), True),\n",
    "    StructField('AMT OVERDUE - HIST', StringType(), True), \n",
    "    StructField('AMT PAID - HIST', StringType(), True),\n",
    "    StructField('TENURE', StringType(), True), \n",
    "    StructField('ts', TimestampType(), True)            \n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_cus = ArrayType(StructType([    \n",
    "    StructField('ID', StringType(), True), \n",
    "    StructField('Frequency', StringType(), True),\n",
    "    StructField('InstlmentMode', StringType(), True), \n",
    "    StructField('LoanStatus', StringType(), True),\n",
    "    StructField('PaymentMode', StringType(), True), \n",
    "    StructField('BranchID', StringType(), True),\n",
    "    StructField('Area', StringType(), True), \n",
    "    StructField('Tenure', StringType(), True),\n",
    "    StructField('AssetCost', StringType(), True), \n",
    "    StructField('AmountFinance', StringType(), True),\n",
    "    StructField('DisbursalAmount', StringType(), True),\n",
    "    StructField('EMI', StringType(), True), \n",
    "    StructField('DisbursalDate', StringType(), True),\n",
    "    StructField('MaturityDAte', StringType(), True), \n",
    "    StructField('AuthDate', StringType(), True),\n",
    "    StructField('AssetID', StringType(), True), \n",
    "    StructField('ManufacturerID', StringType(), True), \n",
    "    StructField('SupplierID', StringType(), True),\n",
    "    StructField('LTV', StringType(), True), \n",
    "    StructField('SEX', StringType(), True),\n",
    "    StructField('AGE', StringType(), True), \n",
    "    StructField('MonthlyIncome', StringType(), True),\n",
    "    StructField('City', StringType(), True), \n",
    "    StructField('State', StringType(), True),\n",
    "    StructField('ZiPCODE', StringType(), True), \n",
    "    StructField('Top-up Month', StringType(), True),\n",
    "    StructField('ts', TimestampType(), True)            \n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bu =df_bu.select(F.from_json(F.col(\"value\").cast(\"string\"), schema_bu).alias('parsed_value'))\n",
    "df_bu = df_bu.select(F.explode(F.col(\"parsed_value\")).alias('unnested_value'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cus =df_cus.select(F.from_json(F.col(\"value\").cast(\"string\"), schema_cus).alias('parsed_value'))\n",
    "df_cus = df_cus.select(F.explode(F.col(\"parsed_value\")).alias('unnested_value'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- unnested_value: struct (nullable = true)\n",
      " |    |-- ID: string (nullable = true)\n",
      " |    |-- SELF-INDICATOR: string (nullable = true)\n",
      " |    |-- MATCH-TYPE: string (nullable = true)\n",
      " |    |-- ACCT-TYPE: string (nullable = true)\n",
      " |    |-- CONTRIBUTOR-TYPE: string (nullable = true)\n",
      " |    |-- DATE-REPORTED: string (nullable = true)\n",
      " |    |-- OWNERSHIP-IND: string (nullable = true)\n",
      " |    |-- ACCOUNT-STATUS: string (nullable = true)\n",
      " |    |-- DISBURSED-DT: string (nullable = true)\n",
      " |    |-- CLOSE-DT: string (nullable = true)\n",
      " |    |-- LAST-PAYMENT-DATE: string (nullable = true)\n",
      " |    |-- CREDIT-LIMIT/SANC AMT: string (nullable = true)\n",
      " |    |-- DISBURSED-AMT/HIGH CREDIT: string (nullable = true)\n",
      " |    |-- INSTALLMENT-AMT: string (nullable = true)\n",
      " |    |-- CURRENT-BAL: string (nullable = true)\n",
      " |    |-- INSTALLMENT-FREQUENCY: string (nullable = true)\n",
      " |    |-- OVERDUE-AMT: string (nullable = true)\n",
      " |    |-- WRITE-OFF-AMT: string (nullable = true)\n",
      " |    |-- ASSET_CLASS: string (nullable = true)\n",
      " |    |-- REPORTED DATE - HIST: string (nullable = true)\n",
      " |    |-- DPD - HIST: string (nullable = true)\n",
      " |    |-- CUR BAL - HIST: string (nullable = true)\n",
      " |    |-- AMT OVERDUE - HIST: string (nullable = true)\n",
      " |    |-- AMT PAID - HIST: string (nullable = true)\n",
      " |    |-- TENURE: string (nullable = true)\n",
      " |    |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bu.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- unnested_value: struct (nullable = true)\n",
      " |    |-- ID: string (nullable = true)\n",
      " |    |-- Frequency: string (nullable = true)\n",
      " |    |-- InstlmentMode: string (nullable = true)\n",
      " |    |-- LoanStatus: string (nullable = true)\n",
      " |    |-- PaymentMode: string (nullable = true)\n",
      " |    |-- BranchID: string (nullable = true)\n",
      " |    |-- Area: string (nullable = true)\n",
      " |    |-- Tenure: string (nullable = true)\n",
      " |    |-- AssetCost: string (nullable = true)\n",
      " |    |-- AmountFinance: string (nullable = true)\n",
      " |    |-- DisbursalAmount: string (nullable = true)\n",
      " |    |-- EMI: string (nullable = true)\n",
      " |    |-- DisbursalDate: string (nullable = true)\n",
      " |    |-- MaturityDAte: string (nullable = true)\n",
      " |    |-- AuthDate: string (nullable = true)\n",
      " |    |-- AssetID: string (nullable = true)\n",
      " |    |-- ManufacturerID: string (nullable = true)\n",
      " |    |-- SupplierID: string (nullable = true)\n",
      " |    |-- LTV: string (nullable = true)\n",
      " |    |-- SEX: string (nullable = true)\n",
      " |    |-- AGE: string (nullable = true)\n",
      " |    |-- MonthlyIncome: string (nullable = true)\n",
      " |    |-- City: string (nullable = true)\n",
      " |    |-- State: string (nullable = true)\n",
      " |    |-- ZiPCODE: string (nullable = true)\n",
      " |    |-- Top-up Month: string (nullable = true)\n",
      " |    |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cus.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bu = df_bu.select(\n",
    "                    F.col(\"unnested_value.ID\").alias(\"ID\"),\n",
    "                    F.col(\"unnested_value.SELF-INDICATOR\").alias(\"SELF-INDICATOR\"),\n",
    "                    F.col(\"unnested_value.MATCH-TYPE\").alias(\"MATCH-TYPE\"),\n",
    "                    F.col(\"unnested_value.ACCT-TYPE\").alias(\"ACCT-TYPE\"),\n",
    "                    F.col(\"unnested_value.CONTRIBUTOR-TYPE\").alias(\"CONTRIBUTOR-TYPE\"),\n",
    "                    F.col(\"unnested_value.DATE-REPORTED\").alias(\"DATE-REPORTED\"),\n",
    "                    F.col(\"unnested_value.OWNERSHIP-IND\").alias(\"OWNERSHIP-IND\"),\n",
    "                    F.col(\"unnested_value.ACCOUNT-STATUS\").alias(\"ACCOUNT-STATUS\"),\n",
    "                    F.col(\"unnested_value.DISBURSED-DT\").alias(\"DISBURSED-DT\"),\n",
    "                    F.col(\"unnested_value.CLOSE-DT\").alias(\"CLOSE-DT\"),\n",
    "                    F.col(\"unnested_value.LAST-PAYMENT-DATE\").alias(\"LAST-PAYMENT-DATE\"),\n",
    "                    F.col(\"unnested_value.CREDIT-LIMIT/SANC AMT\").alias(\"CREDIT-LIMIT/SANC AMT\"),\n",
    "                    F.col(\"unnested_value.DISBURSED-AMT/HIGH CREDIT\").alias(\"DISBURSED-AMT/HIGH CREDIT\"),\n",
    "                    F.col(\"unnested_value.INSTALLMENT-AMT\").alias(\"INSTALLMENT-AMT\"),\n",
    "                    F.col(\"unnested_value.CURRENT-BAL\").alias(\"CURRENT-BAL\"),\n",
    "                    F.col(\"unnested_value.INSTALLMENT-FREQUENCY\").alias(\"INSTALLMENT-FREQUENCY\"),\n",
    "                    F.col(\"unnested_value.OVERDUE-AMT\").alias(\"OVERDUE-AMT\"),\n",
    "                    F.col(\"unnested_value.WRITE-OFF-AMT\").alias(\"WRITE-OFF-AMT\"),\n",
    "                    F.col(\"unnested_value.ASSET_CLASS\").alias(\"ASSET_CLASS\"),\n",
    "                    F.col(\"unnested_value.REPORTED DATE - HIST\").alias(\"REPORTED DATE - HIST\"),\n",
    "                    F.col(\"unnested_value.DPD - HIST\").alias(\"DPD - HIST\"),\n",
    "                    F.col(\"unnested_value.CUR BAL - HIST\").alias(\"CUR BAL - HIST\"),\n",
    "                    F.col(\"unnested_value.AMT OVERDUE - HIST\").alias(\"AMT OVERDUE - HIST\"),\n",
    "                    F.col(\"unnested_value.AMT PAID - HIST\").alias(\"AMT PAID - HIST\"),\n",
    "                    F.col(\"unnested_value.TENURE\").alias(\"TENURE\"),\n",
    "                    F.col(\"unnested_value.ts\").alias(\"ts\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cus = df_cus.select(\n",
    "                    F.col(\"unnested_value.ID\").alias(\"ID\"),\n",
    "                    F.col(\"unnested_value.Frequency\").alias(\"Frequency\"),\n",
    "                    F.col(\"unnested_value.InstlmentMode\").alias(\"InstlmentMode\"),\n",
    "                    F.col(\"unnested_value.LoanStatus\").alias(\"LoanStatus\"),\n",
    "                    F.col(\"unnested_value.PaymentMode\").alias(\"PaymentMode\"),\n",
    "                    F.col(\"unnested_value.BranchID\").alias(\"BranchID\"),\n",
    "                    F.col(\"unnested_value.Area\").alias(\"Area\"),\n",
    "                    F.col(\"unnested_value.Tenure\").alias(\"Tenure\"),\n",
    "                    F.col(\"unnested_value.AssetCost\").alias(\"AssetCost\"),\n",
    "                    F.col(\"unnested_value.AmountFinance\").alias(\"AmountFinance\"),\n",
    "                    F.col(\"unnested_value.DisbursalAmount\").alias(\"DisbursalAmount\"),\n",
    "                    F.col(\"unnested_value.EMI\").alias(\"EMI\"),\n",
    "                    F.col(\"unnested_value.DisbursalDate\").alias(\"DisbursalDate\"),\n",
    "                    F.col(\"unnested_value.MaturityDAte\").alias(\"MaturityDAte\"),\n",
    "                    F.col(\"unnested_value.AuthDate\").alias(\"AuthDate\"),\n",
    "                    F.col(\"unnested_value.AssetID\").alias(\"AssetID\"),\n",
    "                    F.col(\"unnested_value.ManufacturerID\").alias(\"ManufacturerID\"),\n",
    "                    F.col(\"unnested_value.SupplierID\").alias(\"SupplierID\"),\n",
    "                    F.col(\"unnested_value.LTV\").alias(\"LTV\"),\n",
    "                    F.col(\"unnested_value.SEX\").alias(\"SEX\"),\n",
    "                    F.col(\"unnested_value.AGE\").alias(\"AGE\"),\n",
    "                    F.col(\"unnested_value.MonthlyIncome\").alias(\"MonthlyIncome\"),\n",
    "                    F.col(\"unnested_value.City\").alias(\"City\"),\n",
    "                    F.col(\"unnested_value.State\").alias(\"State\"),\n",
    "                    F.col(\"unnested_value.ZiPCODE\").alias(\"ZiPCODE\"),\n",
    "                    F.col(\"unnested_value.Top-up Month\").alias(\"Top-up Month\"),\n",
    "                    F.col(\"unnested_value.ts\").alias(\"ts\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Then the streaming data format should be transformed into the proper formats following the metadata file schema, similar to assignment 2A. Then use 'ts' column as the watermark and set the delay threshold to 5 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove comma**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_col = ['CREDIT-LIMIT/SANC AMT', 'DISBURSED-AMT/HIGH CREDIT', 'INSTALLMENT-AMT', 'CURRENT-BAL', 'OVERDUE-AMT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_correct_id(value):\n",
    "    \n",
    "    # @param: value: contain row in 1 columns\n",
    "\n",
    "    try:\n",
    "        # We checck if value can replace comma\n",
    "        value_reg = value.replace(\",\",\"\")\n",
    "        return value_reg\n",
    "    except:\n",
    "        # if not we return the same format\n",
    "        return value\n",
    "\n",
    "\n",
    "# create UDF \n",
    "convert_num_udf = udf(lookup_correct_id, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop the whole column\n",
    "for cols in comma_col:\n",
    "    \n",
    "    # Apply UDF to change each column with comma values\n",
    "    df_bu = df_bu.withColumn(cols, convert_num_udf(col(cols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove month in INSTALLMENT-AMT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "df_bu = df_bu.withColumn(\"INSTALLMENT-AMT\",regexp_replace(col(\"INSTALLMENT-AMT\"), \"/[a-zA-Z]*\",\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cast specific column to integer and double type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast type in df_bu\n",
    "df_bu = df_bu.withColumn(\"ID\",col(\"ID\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"CREDIT-LIMIT/SANC AMT\",col(\"CREDIT-LIMIT/SANC AMT\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"DISBURSED-AMT/HIGH CREDIT\",col(\"DISBURSED-AMT/HIGH CREDIT\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"INSTALLMENT-AMT\",col(\"INSTALLMENT-AMT\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"CURRENT-BAL\",col(\"CURRENT-BAL\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"OVERDUE-AMT\",col(\"OVERDUE-AMT\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"WRITE-OFF-AMT\",col(\"WRITE-OFF-AMT\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"TENURE\",col(\"TENURE\").cast(IntegerType())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast type in df_cus\n",
    "df_cus = df_cus.withColumn(\"ID\",col(\"ID\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"Tenure\",col(\"Tenure\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"AssetCost\",col(\"AssetCost\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"AmountFinance\",col(\"AmountFinance\").cast(DoubleType())) \\\n",
    "                    .withColumn(\"DisbursalAmount\",col(\"DisbursalAmount\").cast(DoubleType())) \\\n",
    "                    .withColumn(\"LTV\",col(\"LTV\").cast(DoubleType())) \\\n",
    "                    .withColumn(\"AGE\",col(\"AGE\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"MonthlyIncome\",col(\"MonthlyIncome\").cast(DoubleType())) \\\n",
    "                    .withColumn(\"ZiPCODE\",col(\"ZiPCODE\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"EMI\",col(\"EMI\").cast(DoubleType())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**use 'ts' column as the watermark and set the delay threshold to 5 seconds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bu = df_bu \\\n",
    "    .withWatermark(\"ts\", \"5 seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Group the bureau stream based on ID with 30 seconds window duration, similar to assignment 2A(same rule for sum and dist).\n",
    "- Transform the “SELF-INDICATOR” column’s values. If the value is true, then convert to 1, if the value is false, then convert to 0.\n",
    "- sum the rows for numeric type columns, count distinct values for other columns with other data types, and rename them with the postfix like '_sum' or '_dist'. (For example, we did the sum function based on the 'HIGH CREDIT', and the new column’s name will be 'HIGH CREDIT_sum').\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform SELF-INDICATOR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "def convert_indicator(value):\n",
    "    # @param: value: contain row in 1 columns\n",
    "    try:\n",
    "        # We checck if value equal true replace 1\n",
    "        if value == \"true\":\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_indicator_udf = udf(convert_indicator, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bu = df_bu.withColumn(\"SELF-INDICATOR\",convert_indicator_udf(col(\"SELF-INDICATOR\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bu = df_bu.withColumn(\"SELF-INDICATOR\", col(\"SELF-INDICATOR\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sum and count distinct values in each column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_avoid = [\"ID\", \"Top-up Month\"]\n",
    "\n",
    "# column numeric for bureau data frame\n",
    "bu_num_col = [item[0] for item in df_bu.dtypes if item[1].startswith('int') or item[1].startswith('double')]\n",
    "bu_num_col = [item for item in bu_num_col if item not in column_avoid]\n",
    "\n",
    "# column categories for bureau data frame\n",
    "bu_str_col = [item[0] for item in df_bu.dtypes if item[1].startswith('string')]\n",
    "bu_str_col = [item for item in bu_str_col if item not in column_avoid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create aggregation sum for numeric column in bureau dataframe\n",
    "exprs_num = [sum(x) for x in bu_num_col]\n",
    "exprs_num_name = [str(i) + \"_sum\" for i in bu_num_col]\n",
    "\n",
    "# create aggregation countdistinct for numeric column in bureau dataframe\n",
    "exprs_str = [approx_count_distinct(x) for x in bu_str_col]\n",
    "exprs_str_name = [str(i) + \"_dist\" for i in bu_str_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bu = df_bu\\\n",
    "    .groupBy(window(df_bu.ts, \"30 seconds\"), \"ID\")\\\n",
    "    .agg(*exprs_num, *exprs_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "exprs_name = [\"window\"] + [\"ID\"] + exprs_num_name + exprs_str_name\n",
    "# change column name of group_sum_dist\n",
    "count = 0\n",
    "for field in df_bu.schema.fields:\n",
    "    df_bu = df_bu.withColumnRenamed(field.name, exprs_name[count])\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Create new columns named 'window_start' and 'window_end' which are the window’s start time and end time in 2.4. Then inner join the 2 streams based on 'ID', and only customer data received between the window time are accepted. For example, customer data ID '3' received at 10:00, and only when the window of corresponding bureau data contains 10:00(like window start: 9:59, end: 10:00), then this data is accepted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select new column window_start and window_end**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bu = df_bu.select(F.col(\"window.start\").alias(\"window_start\"),\n",
    "                F.col(\"window.end\").alias(\"window_end\"),\n",
    "                F.col(\"ID\"),\n",
    "                F.col(\"CREDIT-LIMIT/SANC AMT_sum\"),\n",
    "                F.col(\"DISBURSED-AMT/HIGH CREDIT_sum\"),\n",
    "                F.col(\"INSTALLMENT-AMT_sum\"),\n",
    "                F.col(\"CURRENT-BAL_sum\"),\n",
    "                F.col(\"OVERDUE-AMT_sum\"),\n",
    "                F.col(\"WRITE-OFF-AMT_sum\"),\n",
    "            F.col(\"TENURE_sum\"),\n",
    "            F.col(\"SELF-INDICATOR_sum\"),\n",
    "            F.col(\"MATCH-TYPE_dist\"),\n",
    "            F.col(\"ACCT-TYPE_dist\"),\n",
    "            F.col(\"CONTRIBUTOR-TYPE_dist\"),\n",
    "            F.col(\"DATE-REPORTED_dist\"),\n",
    "            F.col(\"OWNERSHIP-IND_dist\"),\n",
    "            F.col(\"ACCOUNT-STATUS_dist\"),\n",
    "            F.col(\"DISBURSED-DT_dist\"),\n",
    "            F.col(\"CLOSE-DT_dist\"),\n",
    "            F.col(\"LAST-PAYMENT-DATE_dist\"),\n",
    "            F.col(\"INSTALLMENT-FREQUENCY_dist\"),\n",
    "            F.col(\"ASSET_CLASS_dist\"),\n",
    "            F.col(\"REPORTED DATE - HIST_dist\"),\n",
    "            F.col(\"DPD - HIST_dist\"),\n",
    "            F.col(\"CUR BAL - HIST_dist\"),\n",
    "            F.col(\"AMT OVERDUE - HIST_dist\"),\n",
    "            F.col(\"AMT PAID - HIST_dist\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join 2 dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change ID in df_bu\n",
    "df_bu = df_bu.withColumnRenamed(\"ID\", \"ID1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join dataframe\n",
    "df_join = df_bu.join(\n",
    "  df_cus,\n",
    "  expr(\"\"\"\n",
    "    ID = ID1 AND\n",
    "    ts >= window_start AND\n",
    "    ts <= window_end\n",
    "    \"\"\"),\"inner\"\n",
    ")\n",
    "\n",
    "# drop column ID1\n",
    "df_join = df_join.drop(col(\"ID1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window_start: timestamp (nullable = true)\n",
      " |-- window_end: timestamp (nullable = true)\n",
      " |-- CREDIT-LIMIT/SANC AMT_sum: long (nullable = true)\n",
      " |-- DISBURSED-AMT/HIGH CREDIT_sum: long (nullable = true)\n",
      " |-- INSTALLMENT-AMT_sum: long (nullable = true)\n",
      " |-- CURRENT-BAL_sum: long (nullable = true)\n",
      " |-- OVERDUE-AMT_sum: long (nullable = true)\n",
      " |-- WRITE-OFF-AMT_sum: long (nullable = true)\n",
      " |-- TENURE_sum: long (nullable = true)\n",
      " |-- SELF-INDICATOR_sum: long (nullable = true)\n",
      " |-- MATCH-TYPE_dist: long (nullable = false)\n",
      " |-- ACCT-TYPE_dist: long (nullable = false)\n",
      " |-- CONTRIBUTOR-TYPE_dist: long (nullable = false)\n",
      " |-- DATE-REPORTED_dist: long (nullable = false)\n",
      " |-- OWNERSHIP-IND_dist: long (nullable = false)\n",
      " |-- ACCOUNT-STATUS_dist: long (nullable = false)\n",
      " |-- DISBURSED-DT_dist: long (nullable = false)\n",
      " |-- CLOSE-DT_dist: long (nullable = false)\n",
      " |-- LAST-PAYMENT-DATE_dist: long (nullable = false)\n",
      " |-- INSTALLMENT-FREQUENCY_dist: long (nullable = false)\n",
      " |-- ASSET_CLASS_dist: long (nullable = false)\n",
      " |-- REPORTED DATE - HIST_dist: long (nullable = false)\n",
      " |-- DPD - HIST_dist: long (nullable = false)\n",
      " |-- CUR BAL - HIST_dist: long (nullable = false)\n",
      " |-- AMT OVERDUE - HIST_dist: long (nullable = false)\n",
      " |-- AMT PAID - HIST_dist: long (nullable = false)\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Frequency: string (nullable = true)\n",
      " |-- InstlmentMode: string (nullable = true)\n",
      " |-- LoanStatus: string (nullable = true)\n",
      " |-- PaymentMode: string (nullable = true)\n",
      " |-- BranchID: string (nullable = true)\n",
      " |-- Area: string (nullable = true)\n",
      " |-- Tenure: integer (nullable = true)\n",
      " |-- AssetCost: integer (nullable = true)\n",
      " |-- AmountFinance: double (nullable = true)\n",
      " |-- DisbursalAmount: double (nullable = true)\n",
      " |-- EMI: double (nullable = true)\n",
      " |-- DisbursalDate: string (nullable = true)\n",
      " |-- MaturityDAte: string (nullable = true)\n",
      " |-- AuthDate: string (nullable = true)\n",
      " |-- AssetID: string (nullable = true)\n",
      " |-- ManufacturerID: string (nullable = true)\n",
      " |-- SupplierID: string (nullable = true)\n",
      " |-- LTV: double (nullable = true)\n",
      " |-- SEX: string (nullable = true)\n",
      " |-- AGE: integer (nullable = true)\n",
      " |-- MonthlyIncome: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- ZiPCODE: integer (nullable = true)\n",
      " |-- Top-up Month: string (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Persist the above result in parquet format.(When you save the data to parquet format,you need to rename “Top-up Month” to “Top-up_Month” first. And only keep these columns “ID”, “window_start”, “window_end”, “ts”, “Top-up_Month”) Renaming “Top-up Month” only happen in this question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change name in top-up month\n",
    "df_join = df_join.withColumnRenamed(\"Top-up Month\", \"Top-up_Month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop na if have\n",
    "df_join = df_join.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select column to save\n",
    "df_join1 = df_join.select(\"ID\", \"window_start\", \"window_end\",\"ts\",\"Top-up_Month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start parquet\n",
    "query_file_join = df_join1.writeStream.format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"parquet/df_join\")\\\n",
    "        .option(\"checkpointLocation\", \"parquet/df_join/checkpoint\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop parquet\n",
    "query_file_join.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- window_start: timestamp (nullable = true)\n",
      " |-- window_end: timestamp (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- Top-up_Month: string (nullable = true)\n",
      "\n",
      "+-----+-------------------+-------------------+-------------------+-----------------+\n",
      "|   ID|       window_start|         window_end|                 ts|     Top-up_Month|\n",
      "+-----+-------------------+-------------------+-------------------+-----------------+\n",
      "|17615|2022-10-18 04:19:30|2022-10-18 04:20:00|2022-10-18 04:19:42|No Top-up Service|\n",
      "|17645|2022-10-18 04:19:30|2022-10-18 04:20:00|2022-10-18 04:19:52|No Top-up Service|\n",
      "|17582|2022-10-18 04:19:30|2022-10-18 04:20:00|2022-10-18 04:19:37|No Top-up Service|\n",
      "|17740|2022-10-18 04:20:00|2022-10-18 04:20:30|2022-10-18 04:20:19|No Top-up Service|\n",
      "|17881|2022-10-18 04:20:30|2022-10-18 04:21:00|2022-10-18 04:20:56|No Top-up Service|\n",
      "+-----+-------------------+-------------------+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the saved parquet data\n",
    "query_file_join_read = spark.read.parquet(\"parquet/df_join\")\n",
    "query_file_join_read.printSchema()\n",
    "query_file_join_read.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Load the machine learning models given and use the model to predict whether users will be joining the top-up service. Save the results in parquet format. (When you save the data to parquet format,you need to rename “Top-up Month” to “Top-up_Month” first. And only keep these columns “ID”, “window_start”, “window_end”, “ts”, “prediction”, “Top-up_Month”) Renaming “Top-up Month” will happen in this question as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "model = PipelineModel.load('topup_pipeline_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(df_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_store = prediction.select(\"ID\", \"window_start\", \"window_end\", \"prediction\", \"Top-up_Month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = prediction_store.writeStream.format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"parquet/model\")\\\n",
    "        .option(\"checkpointLocation\", \"parquet/model/checkpoint\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- window_start: timestamp (nullable = true)\n",
      " |-- window_end: timestamp (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      " |-- Top-up_Month: string (nullable = true)\n",
      "\n",
      "+-----+-------------------+-------------------+----------+-----------------+\n",
      "|   ID|       window_start|         window_end|prediction|     Top-up_Month|\n",
      "+-----+-------------------+-------------------+----------+-----------------+\n",
      "|18424|2022-10-18 04:23:30|2022-10-18 04:24:00|       0.0|No Top-up Service|\n",
      "|18426|2022-10-18 04:23:30|2022-10-18 04:24:00|       1.0|No Top-up Service|\n",
      "|18643|2022-10-18 04:24:30|2022-10-18 04:25:00|       0.0|No Top-up Service|\n",
      "|18676|2022-10-18 04:24:30|2022-10-18 04:25:00|       0.0|No Top-up Service|\n",
      "|18681|2022-10-18 04:24:30|2022-10-18 04:25:00|       0.0|No Top-up Service|\n",
      "|18665|2022-10-18 04:24:30|2022-10-18 04:25:00|       0.0|No Top-up Service|\n",
      "|18808|2022-10-18 04:25:00|2022-10-18 04:25:30|       0.0|No Top-up Service|\n",
      "|18921|2022-10-18 04:25:30|2022-10-18 04:26:00|       0.0|No Top-up Service|\n",
      "|18943|2022-10-18 04:25:30|2022-10-18 04:26:00|       0.0|No Top-up Service|\n",
      "|18404|2022-10-18 04:23:00|2022-10-18 04:23:30|       1.0|      > 48 Months|\n",
      "|18472|2022-10-18 04:23:30|2022-10-18 04:24:00|       0.0|      > 48 Months|\n",
      "|18440|2022-10-18 04:23:30|2022-10-18 04:24:00|       1.0|     18-24 Months|\n",
      "|18566|2022-10-18 04:24:00|2022-10-18 04:24:30|       1.0|     12-18 Months|\n",
      "|18677|2022-10-18 04:24:30|2022-10-18 04:25:00|       1.0|     12-18 Months|\n",
      "|18865|2022-10-18 04:25:30|2022-10-18 04:26:00|       0.0|      > 48 Months|\n",
      "|19096|2022-10-18 04:26:30|2022-10-18 04:27:00|       0.0|     24-30 Months|\n",
      "+-----+-------------------+-------------------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the saved parquet data\n",
    "model_file_read = spark.read.parquet(\"parquet/model\")\n",
    "model_file_read.printSchema()\n",
    "model_file_read.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8. Only keep the customer predicted as our target customers (willing to join the top-up service). Normally, we should only keep “Top-up=1”. But due to the limited performance of our VM, if your process is extremely slow, you can abandon the filter and keep all of the data. Then for each batch, show the epoch id and count of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predicted_value = prediction.select(\"ID\", \"window_start\", \"window_end\", \"prediction\", \"Top-up_Month\", \"State\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreach_batch_function(df, epoch_id):\n",
    "    # Transform and write batchDF\n",
    "    if df.count() <= 0:\n",
    "        print(f\"bactch_id: {epoch_id}, count: {df.count()}\")\n",
    "    else:\n",
    "        print(f\"bactch_id: {epoch_id}, count: {df.count()}\")\n",
    "        \n",
    "        # filter predition and count state\n",
    "        df = df.select(\"window_end\", \"State\", \"prediction\").filter(\"prediction == 1\").groupBy(\"window_end\", \"State\").agg(F.count(\"State\").alias(\"count\"))\n",
    "        \n",
    "        # convert state and count column to json format\n",
    "        df = df.groupBy(\"window_end\").agg(\n",
    "        F.collect_list(\n",
    "            to_json(struct(\"State\", \"count\"))).alias(\"count\")\n",
    "        )\n",
    "        \n",
    "        # change name column\n",
    "        df = df.withColumnRenamed(\"window_end\", \"key\").withColumnRenamed(\"count\", \"value\").sort(\"key\")\n",
    "\n",
    "\n",
    "        \n",
    "        # write static data with parquet\n",
    "        write = df.write.mode('append').format('parquet').option(\"path\", \"parquet/plot\")\\\n",
    "        .option(\"checkpointLocation\", \"parquet/plot/checkpoint\")\\\n",
    "        .save()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # show dataframe\n",
    "        if df.count() > 0:\n",
    "            df.show(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "query1 = predicted_value.writeStream\\\n",
    "        .foreachBatch(foreach_batch_function)\\\n",
    "        .trigger(processingTime='5 seconds')\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data parquet\n",
    "query_file_sink_df = spark.read.parquet(\"parquet/plot\")\n",
    "\n",
    "# get schema\n",
    "schema_df = query_file_sink_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read stream schema\n",
    "paquet_df = spark.readStream.schema(schema_df).parquet(\"parquet/plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bactch_id: 5, count: 1\n",
      "bactch_id: 6, count: 4\n",
      "bactch_id: 7, count: 6\n",
      "bactch_id: 8, count: 4\n",
      "+-------------------+--------------------+\n",
      "|                key|               value|\n",
      "+-------------------+--------------------+\n",
      "|2022-10-18 04:34:30|[{\"State\":\"RAJAST...|\n",
      "|2022-10-18 04:35:00|[{\"State\":\"RAJAST...|\n",
      "+-------------------+--------------------+\n",
      "\n",
      "bactch_id: 9, count: 13\n",
      "+-------------------+--------------------+\n",
      "|                key|               value|\n",
      "+-------------------+--------------------+\n",
      "|2022-10-18 04:35:30|[{\"State\":\"RAJAST...|\n",
      "+-------------------+--------------------+\n",
      "\n",
      "bactch_id: 10, count: 12\n"
     ]
    }
   ],
   "source": [
    "# write stream paquet\n",
    "query_parquet = paquet_df\\\n",
    "    .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"topic\", \"final_best\") \\\n",
    "    .option(\"checkpointLocation\", \"final_best\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop both query\n",
    "query1.stop()\n",
    "query_parquet.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
